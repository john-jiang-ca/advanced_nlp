import jieba
text= "自然语言处理很有趣" # it means natural language processing is interesting
segmentations = jieba.cut(text)


list(segmentations)


import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')


stopwords.words('english')[:10]


from pyspark.ml.feature import Imputer, OneHotEncoder, RobustScaler, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
from datetime import datetime
from pyspark.ml import Transformer
from pyspark import keyword_only
from pyspark.ml.param.shared import HasOutputCol, Param, Params, HasInputCol
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
import pandas as pd
spark = SparkSession.builder \
    .appName("Spark NLP clean")\
    .master("local[12]")\
    .config("spark.kryoserializer.buffer.max", "2000M")\
    .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.0").getOrCreate()
pd.set_option('display.max_colwidth', None)
pd.set_option('max_colwidth',500)

df = spark.read.format('csv') \
  .option("inferSchema", True) \
  .option("header", True) \
  .option("sep", ',') \
  .load('./bbc-text.csv')


# Display first 10 rows of the content
df.limit(10).toPandas()


import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

document_assembler = DocumentAssembler() \
    .setInputCol("text") \
    .setOutputCol("document")

sentenceDetector = SentenceDetector()\
    .setInputCols(['document'])\
    .setOutputCol('sentences')

tokenizer = Tokenizer() \
  .setInputCols(["sentences"]) \
  .setOutputCol("token")

# clean tokens 
normalizer = Normalizer() \
    .setInputCols(["token"]) \
    .setOutputCol("normalized")

stopwords_cleaner = StopWordsCleaner()\
      .setInputCols("normalized")\
      .setOutputCol("cleanTokens")\
      .setCaseSensitive(False)

finisher = Finisher() \
    .setInputCols(["cleanTokens"]) \
    .setOutputCols(["token_features"]) \
    .setOutputAsArray(True) \
    .setCleanAnnotations(False)


pipeline = Pipeline(
    stages=[document_assembler,
            sentenceDetector,
            tokenizer,
            normalizer,
            stopwords_cleaner, 
            finisher]).fit(df)
result = pipeline.transform(df)

cleaned = result.select(['text', 'token_features'])


result.limit(3).toPandas()


get_ipython().getoutput("wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt")


document_assembler = DocumentAssembler() \
    .setInputCol("text")\
    .setOutputCol("document")

tokenizer = Tokenizer()\
  .setInputCols(["document"]) \
  .setOutputCol("token")

normalizer = Normalizer() \
    .setInputCols(["token"]) \
    .setOutputCol("normalized")

stopwords_cleaner = StopWordsCleaner()\
      .setInputCols("normalized")\
      .setOutputCol("cleanTokens")\
      .setCaseSensitive(False)

stemmer = Stemmer() \
    .setInputCols(["cleanTokens"]) \
    .setOutputCol("stem")


lemmatizer = Lemmatizer() \
    .setInputCols(["stem"]) \
    .setOutputCol("lemma") \
    .setDictionary("./AntBNC_lemmas_ver_001.txt", value_delimiter ="\t", key_delimiter = "->")

finisher = Finisher() \
    .setInputCols(["lemma"]) \
    .setOutputCols(["token_features"]) \
    .setOutputAsArray(True) \
    .setCleanAnnotations(False)

pipeline = Pipeline(
    stages=[document_assembler, 
            tokenizer,
            normalizer,
            stopwords_cleaner,
            stemmer,
            lemmatizer,
            finisher]).fit(df)
result = pipeline.transform(df)

result.limit(3).toPandas()


from pyspark.sql import functions as F
result_df = result.select(F.explode(F.arrays_zip('cleanTokens.result', 'stem.result',  'lemma.result')).alias("cols")) \
.select(F.expr("cols['0']").alias("token"),
        F.expr("cols['1']").alias("stem"),
        F.expr("cols['2']").alias("lemma")).where(F.col("stem")!=F.col("lemma")).where(F.col("stem")!=F.col("token")).limit(20).toPandas()

result_df
# result_df.head(50)



