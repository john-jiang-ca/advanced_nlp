{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2654f-1eb3-4675-b1a7-a151eacf3bc1",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "NLP = NLU (understanding) + NLG (generation)\n",
    "\n",
    "## Typical applications:\n",
    "- Smart Q&A system\n",
    "- Text generation (to generate report, ads, abstrct etc.)\n",
    "- Machine translation (we usually do this)\n",
    "- Sentimental analysis\n",
    "- chat bot\n",
    "- fake news detection\n",
    "- text classification (e.g. classify news)\n",
    "- Information extraction (unstructure data to structure data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8f3af-b74d-4cb1-9868-cdd7aaad08a2",
   "metadata": {},
   "source": [
    "## 3 dimentions in NLP\n",
    "- Morphology (work level)\n",
    "- Syntax (sentence level, grammer AST)\n",
    "- Semitic (understand the meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5fcde5-6dbc-4ac0-a3e5-c3d15954fc2e",
   "metadata": {},
   "source": [
    "## Tasks in NLP\n",
    "### Word segmentation (for Chinese)\n",
    "A common used library is jieba. For English / French, we do not need it. The accuracy is around 97%\n",
    "### Part of speech (POS) tagging \n",
    "An easier task, the accuracy is around 96% ~ 98%.\n",
    "\n",
    "### Semitic analysis\n",
    "One of the core tasks in NLP. A typical technique is BERT.\n",
    "\n",
    "### Name engity recognition (NER)\n",
    "One of the most important tasks in NLP. It try to extract company name, locatiopn name, etc. \n",
    "\n",
    "### Dependency parsing \n",
    "Analyse the relationship between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5433d9c-62d9-4513-bf5a-a297e5b4cc84",
   "metadata": {},
   "source": [
    "# Basic algorithms we need to know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7ff1b-c058-4a5a-ac7f-7c68cfdb7cd9",
   "metadata": {},
   "source": [
    "## Dynamic text warping (DTW)\n",
    "DTW is used to compute the similarity of two temporal sequences. One typical application of DTW is voice recognition. \n",
    "\n",
    "First considering two sequence with same length, their distance can be described as their Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef68972-06df-46a6-87ad-3d5d05e7aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "x_1 = numpy.array([1, 1, 2, 3])\n",
    "x_2 = numpy.array([3, 3, 2, 3])\n",
    "distance = numpy.linalg.norm(x_1 - x_2)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4c3126-1197-4f54-b29b-90ebb9e754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In real world, the length of the array will be different, e.g.\n",
    "x1 = numpy.array([1,2,2,3])\n",
    "x2 = numpy.array([2,2,2,3,3,1,1,1,2,2,3,4,5,5,1,1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcf3eb-b332-4435-89a9-412fcb1b715b",
   "metadata": {},
   "source": [
    "The core of DTW is compute the mapping between two series. The mapping will be one to many (or many to one). To simplify the question, we can assume that the start and end point are known to us. \n",
    "\n",
    "It's easy to see the core of DTW is a DP problem. Define $\\text{DWT}(i, j)$ as the min mapping distance from $(0, 0)$ to point $(i, j)$. It's easier to see \n",
    "\n",
    "$\\text{DWT}(i, j) = \\min\\limits_{x, y}\\{\\text{DWT}(x, y) + \\text{dist}((x, y), (i, j))\\}$ where $x\\leq i$ and $y \\leq j$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77df178d-376e-4006-b75c-4e50ce78d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def euc_dist(v1, v2):\n",
    "  \"\"\"\n",
    "  define the distance\n",
    "  \"\"\"\n",
    "  return np.abs(v1-v2)\n",
    "\n",
    "\n",
    "def dtw(s, t):\n",
    "  \"\"\"\n",
    "  We will use DP to calculate the minimum distance bwtween the two sequences\n",
    "  s: source sequence\n",
    "  t: target sequence\n",
    "  \"\"\"\n",
    "  m, n = len(s), len(t)\n",
    "  dtw = np.zeros((m, n))\n",
    "  dtw.fill(sys.maxsize)\n",
    "\n",
    "  # init\n",
    "  dtw[0,0] = euc_dist(s[0], t[0])\n",
    "  for i in range (1,m):\n",
    "    dtw[i,0] = dtw[i-1,0] + euc_dist(s[i], t[0])\n",
    "  for i in range (1,n):\n",
    "    dtw[0,i] = dtw[0,i-1] + euc_dist(s[0], t[i])\n",
    "\n",
    "  for i in range(1, m): # dp[][]\n",
    "    for j in range(1,n):\n",
    "      cost = euc_dist(s[i], t[j])\n",
    "      ds = []\n",
    "      ds.append(cost+dtw[i-1, j])\n",
    "      ds.append(cost+dtw[i,j-1])\n",
    "      ds.append(cost+dtw[i-1,j-1])\n",
    "      ds.append(cost + dtw[i-1,j-2] if j>1 else sys.maxsize)\n",
    "      ds.append(cost+dtw[i-2,j-1] if i>2 else sys.maxsize)\n",
    "      dtw[i,j] = min(ds)\n",
    "  return dtw[m-1, n-1]\n",
    "\n",
    "dtw([5,6,9], [5,6,7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed22c5e-5d9f-4d43-9554-10f057323977",
   "metadata": {},
   "source": [
    "## Base line: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb612e-4101-49db-a8c2-e1ae65bfb1a5",
   "metadata": {},
   "source": [
    "### Why we need baseline\n",
    "LR is usually serves as a baseline. \n",
    "\n",
    "Knowing the baseline can help us guess the upper bound. For example, if you get 0.69 accuracy for logistic regression, 0.72 for SVM, 0.73 for single layer NN, then it makes sense that the upper bound is around 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60123f24-852d-4b30-9357-60584974115c",
   "metadata": {},
   "source": [
    "### The math part about LR\n",
    "LR assums the the data subject to _binary_ distribution. By leveraging MLE (maximum likelyhood estimation) and gradient descendent, to classify the data. \n",
    "\n",
    "\n",
    "The expression of LR can be wrote as \n",
    "$$P(y|x; \\boldsymbol{\\theta}, b) = \\frac{1}{1+\\exp^{-z}} = \\frac{1}{1+\\exp^{-(\\boldsymbol{\\omega}^T\\mathbf{x}+b)}}$$. \n",
    "\n",
    "When using LR, the default loss function is log loss:\n",
    "\n",
    "$$\n",
    "l(\\boldsymbol{\\theta}) = \\sum_{i=1}^{T}\\ln P(y_i|x_i | \\boldsymbol{\\theta})\n",
    "$$\n",
    "and the parameter can be estimated by \n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\max_{\\boldsymbol{\\theta}} l(\\boldsymbol{\\theta}) \n",
    "$$\n",
    "\n",
    "Reasons to use log lose in LR:\n",
    "- convex\n",
    "- gradient stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de79ac-b9f5-4726-a1d3-5f8433c4597b",
   "metadata": {},
   "source": [
    "### Feature selection for LR\n",
    "- Need to remove highly related features becuase highly related features will slow down the converge speed and reduce the model's explainability.\n",
    "- We cannot automate the process\n",
    "- We also need to normalize acorss all the features before put data into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5d17b-267e-4565-89e9-ddcb2c033ac4",
   "metadata": {},
   "source": [
    "### Combat overfit in LR\n",
    "Model with large parameters is tend to overfit. \n",
    "\n",
    "- __$l_1$ norm__. We usually avoid $l_1$ norm. The loss function will not be differentiable and the feature selection for $l_1$ norm is really random. As a result, even though it will get us sparse model, we usually avoid using it. Applying $l_1$ norm is equal to use MAP (Maximum a posteriori estimation) to estimate $\\boldsymbol{\\theta}$ by assuming $\\theta$ subject to [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution).\n",
    "\n",
    "-  __$l_2$ norm__. The loss function is differentiable, it will get parameters close to 0. Applying $l_2$ norm is equal to use MAP (Maximum a posteriori estimation) to estimate $\\boldsymbol{\\theta}$ by assuming $\\theta$ subject to Gaussian Distribution.\n",
    "\n",
    "- __Cross features__. A good example is facebook's GBDT+LR\n",
    "\n",
    "- __Discrete feature values__\n",
    "- __Use more data__. Math behind: when having unlimited data, the performance of MLE and MAP will be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb2cf1-d8e9-404b-940c-55370a0a4ff7",
   "metadata": {},
   "source": [
    "### Converge indicators in LR\n",
    "- Loss function does not change much over a few epoches\n",
    "- The parameters does not change much over a few epoches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4367829-79a2-43dc-b034-c0e34729b47d",
   "metadata": {},
   "source": [
    "### Reasons to use LR\n",
    "- Fast and widely supported\n",
    "- Easy to explain\n",
    "- The result is independent from the initialization (LR loss function is convex and we will converge to the same result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e1558-4cc5-4562-b836-ed46a9270dbd",
   "metadata": {},
   "source": [
    "## Naive Baysian in NLP\n",
    "Suitable for text classification, e.g. \n",
    "- identify spam email\n",
    "- topic classification\n",
    "- sentinel analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd958349-0e4f-4a0d-9cf2-a12fbe6c64f1",
   "metadata": {},
   "source": [
    "Core assumption in Naive Baysian: _it assums all features are independent_.\n",
    "\n",
    "When applying naive baysian in NLP (e.g. spam email identification), we need to do some smoothing (one example is _add one smoothing_) in order to get a reasonable likelyhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff225f18-b44e-4f0c-9c5f-5e709eaf41e7",
   "metadata": {},
   "source": [
    "## Constraint optimization\n",
    "\n",
    "### Lagrange multiplier\n",
    "One example of constraint optimization is Lagrange multiplier. Assuming we have following question:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max  & \\;\\;\\;x+y \\\\\n",
    "\\text{s.t.} &\\;\\;\\; x^2+y^2=1 \\\\\n",
    "            &\\;\\;\\; x^3+y \\leq 3\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "after applying lagrange multiplier we can get \n",
    "$f(x, y, \\alpha, \\beta) =  x+y+\\alpha (x^2+y^2-1) + \\beta(x^3+y - 3)$. The optimal solution will subject to\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x}  = 0\\\\\n",
    "\\frac{\\partial f}{\\partial y}  = 0\\\\\n",
    "\\frac{\\partial f}{\\partial \\alpha}  = 0\\\\\n",
    "\\frac{\\partial f}{\\partial \\beta}  = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In general, we can solve both equality-constrained and inequality-constrained using lagrange multiplier. However, in a lot of cases, it is really hard to sovle the origianl lagrange question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6b470-00e9-44e1-a1ab-9b5260a0a299",
   "metadata": {},
   "source": [
    "### Lagrange duality\n",
    "Considering original question\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max & \\;\\;\\;f(x)\\\\\n",
    "\\text{s.t.} &\\;\\;\\;c_i(x) = 0 \\;\\;(i=1, 2, ...,k)\\\\\n",
    "            &\\;\\;\\;h_j(x) \\leq 0 \\;\\;(j=1, 2, ...,l).\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "its lagrange function is defined as \n",
    "\n",
    "$$\n",
    "L(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^{k}\\lambda_ic_i(x) + \\sum_{j=1}^{l}\\mu_jh_j(x).\n",
    "$$\n",
    "\n",
    "The Lagrange Dual function is defined as \n",
    "$$\n",
    "g(\\lambda, \\mu) = \\inf L(x, \\lambda, \\mu).\n",
    "$$\n",
    "\n",
    "It has been proved that the Lagrange Dual is a concave function. Assuming $p^*$ is the solution for origial question and $d^*$ is the solution for Lagrange Dual function, we know for $\\forall \\lambda \\geq 0$, we have $g(\\lambda, \\mu) \\leq p^*$. \n",
    "\n",
    "When it is hard to calculate $p^*$, we can try to get the maximum of $g(\\lambda, \\mu)$ instead. Here we are trying get the maximum of a concave funtion, which is equal to get the minmum of a convex function, i.e. even though we do not know if $f(x)$ is convex or not, we can always convert it to a convex optimization question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff3587-4b3b-46a8-8d84-45739b0ce54f",
   "metadata": {},
   "source": [
    "### Duality gap\n",
    "- Strong duality: $d^* = p^*$\n",
    "- Weak duality: $d^* \\leq p^*$\n",
    "\n",
    "Condition for strong duality:\n",
    "- KKT (refer to [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf))\n",
    "- Convex functions + Slater conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a631bab-f849-4265-89d6-3905a8cbcb05",
   "metadata": {},
   "source": [
    "### Example\n",
    "Refer to tensorflow constraint optimization ([TFCO](https://github.com/google-research/tensorflow_constrained_optimization))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445673d1-0472-4b35-8bc5-b7dfd086dac4",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "There are 3 things to train:\n",
    "- shape of the tree\n",
    "- threshold\n",
    "- node leaf value\n",
    "\n",
    "In general, no matter which library you are using, there are hyperparameters in two categories:\n",
    "- Depth of the tree\n",
    "- Early stopping conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f1b21-18a0-4164-a4ee-d238ea8cd88c",
   "metadata": {},
   "source": [
    "## Ensemble model\n",
    "For _most_ of the time, ensemble model is our __first choice__ in production:\n",
    "- good performance\n",
    "- stable\n",
    "- explainability\n",
    "\n",
    "Ensemble model can reduce variance, and _sometimes_ can also improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65ddad-7e59-49bc-bc75-e644c47da3d9",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bagging model has:\n",
    "- multiple training models\n",
    "- multiple predictions\n",
    "\n",
    "A typical badding model is Random Forest. In Random Forest, we have:\n",
    "- randomized samples\n",
    "- randomized features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dca2a2-0592-44a1-aed2-2434fe743105",
   "metadata": {},
   "source": [
    "### Boost\n",
    "The standard library is `GBDT` or `XGBoost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63e94d-0937-4455-8e50-e5814e824ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
